# Machine Learning
This is a library containing the implementation of the most popular machine learning algorithms, each tailored to address specific challenges and scenarios. From classic decision trees to advanced ensemble learning and support vector machines, here are a range of tools to build, train, and evaluate models.

## Decision Tree
The way to create a new tree is by instantiating the DecisionTree class and calling ID3, which is a method that contains 3 parameters: dataset, features, and depth. The dataset parameter should be a pandas dataframe containing all of the features and the training labels. The features parameter represents a dictionary containing the features as keys and its respective values (like 'Sunny', 'Overcast', 'Rainy' for the Outlook feature) as their value pair. When the method is first called, 'features' contains all the available features and with each recursive call, it takes only a subset of the previous features set. The depth parameter represents the maximum depth of the tree and it is used to limit the size of the tree. With each recursive call, the depth decreases by one, and the tree will either stop when it reaches that depth or it will grow to the maximum possible level if the depth is not yet reached.

## Ensemble Learning
Adaboost can be run by instantiating the Adaboost class and calling the method adaboost, which contains 4 parameters: train_dataset, test_dataset, features, and number_classifiers. The train_dataset parameter is the dataset that will be used to train the algorithm and the test_dataset is the dataset for which the algorithm will predict the results based on the training. The features parameter is a list of all the parameters from the datasets, and number_classifiers defines how many trees we want to use for the Adaboost algorithm.

Bagged Trees can be run by instantiating the BaggedTrees class and calling the method bagging, which contains 4 parameters: train_dataset, test_dataset, features, number_of_trees. These are the same parameters that are used in the Adaboost algorithm.

Random Forest can be run by instantiating the RandomForest class and calling the method random_forest, which contains 5 parameters: train_dataset, test_dataset, features, number_of_trees, number_of_features. These are the same parameters that are used in Adaboost and Bagged Trees, with the only difference being the addition of the parameter number_of_features. This parameter defines how many features we want to use to limit the set of features from which we can split at each iteration.

## Perceptron
Perceptron can be run by instantiating the Perceptron class and calling one of three methods to train the algorithm: train_standard, train_voted or train_averaged. Regardless of what method is chosen to train the model, they all have 3 parameters: train_dataset, epochs, learning_rate. The train_dataset parameter is the dataset that will be used to train the model. The parameter epochs represents the number of times that we want to go through all the examples, updating the weights every time. The parameter learning_rate is used to assign the learning rate that will be used to correct mistakes at each iteration. Depending on what method is used to train the model, we have three methods that are used to predict labels for a test_dataset: predict_standard, predict_voted or predict_averaged. All of them use 2 parameters - a dataset that has no labels (so that the model can predict them) and a list of weights for every feature in the dataset.

## SVM
SVM can be run to solve problems in the primal or dual spaces. The primal model can be run by instantiating the SVM class and calling train_primal. This method has 3 parameters: train_dataset, epochs, learning_rate, and C. The train_dataset parameter is the dataset that will be used to train the model. The epochs parameter represents the number of times that we want to go through all the examples, updating the weights every time. The learning_rate parameter is what determines how quickly (or how slow) the algorithm converges. The C parameter is the regularization term, which determines the tradeoff between maximizing the margin and minimizing the number of outliers. Once the model is trained, we can predict the results by calling predict_primal using the previously instantiated SVM class. This method has 2 parameters: dataset, weights. The dataset parameter takes in the dataset that we are using to predict the labels. The weights parameters is the list of weights that was returned when we trained the model using the train_primal method. 

The same process can be applied to train a model in the dual space, but this is run with the train_dual method instead. This method has 4 parameters: train_dataset, C, kernel, gammas. The train_dataset and C parameters are the same parameters represented in the train_primal method. The kernel parameter is the objective function that is used (like the dot product). The gammas parameter is a list of gamma values corresponding to each of the weights. This parameter is optional. A list of alpha values is returned after the model is trained, and this list can be used to recover the weights and the bias using the methods recover_dual_weights and recover_dual_bias. Once the model is trained, the predict_dual method is used to predict the labels. This method has 6 parameters: dataset, inputs, labels, alphas, bias, kernel. The dataset parameter takes in the dataset that we are using to predict the labels. The inputs and labels parameters represent the X and y values of the dataset, respectively. The alphas parameter is the list of alpha values that was returned from the method that trains the model. The bias parameter is the returned value from the recover_dual_bias method. The kernel is the objective function that is used for the model.

## Neural Networks
Neural networks can be run by instantiating the NeuralNetwork class and calling create_network. This method contains 3 parameters: number_of_inputs, number_of_hidden_neurons, number_of_outputs. The parameter called number_of_inputs determines how many neurons there are in the first layer of the network (Layer 0). The default value for this parameter is 4. The parameter called number_of_hidden_neurons determines how many neurons there are in the second and third layers of the network (Layers 1 and 2). The default value for this parameter is 4. The parameter number_of_outputs determines how many neurons there are in the last layer of the network (Layer 3). The default value for this parameter is 1. Once the network has been initialized, we can train it by calling the train method, which contains 4 parameters: network, train_dataset, learning_rate, epochs. The network parameter takes in the initialized network that has just been created. The train_dataset parameter takes in the dataset that is used to train the model containing both inputs and labels. The learning_rate parameter is a numeric value to define how quickly (or how slowly) the network will converge for training. The epochs parameter determines the number of times that we want to go through all the examples, updating the weights every time. Once the model is trained, we can predict the labels of a dataset using the predict method, which contains 2 parameters: network, dataset. The network parameter is used to pass the network used for training. The dataset parameter contains the X values of the dataset and a blank column for the y values that the method will use for the predictions.